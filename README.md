Large language models (LLMs) have a surprising failure: when trained on facts like “A is B”, they fail to infer “B is A”, which is termed as the Reversal Curse ([Berglund et al. (2024)](https://arxiv.org/abs/2309.12288)). We posit that this failure happens due to existing semantics associated with names in the facts. To address this, we propose a new fine-tuning strategy, Special Token Augmentation. This involves augmenting our dataset with special tokens to control for existing associations. We further investigate the impact of our fine-tuning strategies on the model's prior associations, and find that special token replacement had a minimal impact.
